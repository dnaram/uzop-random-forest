{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c6ad8b9",
   "metadata": {},
   "source": [
    "# Introduction to Data Science\n",
    "\n",
    "## Project: Global Refinement of Random Forest\n",
    "\n",
    "### 1. Introduction\n",
    "\n",
    "This Juptyer Notebook is used as part of Introduction to Data Science course's project task which will present the Global Refinement of Random Forest (https://openaccess.thecvf.com/content_cvpr_2015/papers/Ren_Global_Refinement_of_2015_CVPR_paper.pdf) by Shaoqing Ren et al. In the first part of the project proposed approach from the given paper will be evaluated on standard machine learning tasks:\n",
    "- MNIST: https://archive.ics.uci.edu/ml/machine-learning-databases/mnist-mld/\n",
    "- covtype: https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/\n",
    "\n",
    "for classification and:\n",
    "- abalone\n",
    "- ailerons\n",
    "\n",
    "for regression problems.\n",
    "\n",
    "Later on, the evaluation of the proposed approach will be made on the real world application - namely the one solving the classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c6f682",
   "metadata": {},
   "source": [
    "### 2. Standard machine learning tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e7f701",
   "metadata": {},
   "source": [
    "### 2.1 Classification: MNIST "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bc9f41",
   "metadata": {},
   "source": [
    "Since it will be used for MNIST dataset import from already prepared sources which keras.datasets offers it is necessary to include `tensorflow` library by simply commenting out the next code cell and executing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fd539d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e85495",
   "metadata": {},
   "source": [
    "Here are listed all imports which are going to be used later in the code. It is always possible to return back here later in case that you want to add some others or remove existing ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca2fc246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import zero_one_loss\n",
    "from sklearn.metrics import hinge_loss\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from utils.RefinedRandomForest import RefinedRandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc5eda8",
   "metadata": {},
   "source": [
    "Here comes the importing dataset part. Since it is fairly common MNIST dataset is already present in large share of different libraries - in this case keras.dataset module will be used to importing. For some other dataset used later in the project this will not be the case, so the dataset will have to be downloaded locally and imported from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22410b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape((60000,784))\n",
    "X_test = X_test.reshape((10000,784))\n",
    "\n",
    "X = np.concatenate((X_train, X_test), axis=0)\n",
    "y = np.concatenate((y_train, y_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030ebb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of train examples: {X_train.shape[0]}')\n",
    "print(f'Number of test examples: {y_test.shape[0]}')\n",
    "print(f'Number of features: {len(X_train[0])}')\n",
    "print(f'Number of classes: {len(np.unique(y_test))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dcaed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9):  \n",
    "    plt.subplot(330 + 1 + i)\n",
    "    plt.imshow(np.array(X_train[i]).reshape((28,28)), cmap=plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467e28c4",
   "metadata": {},
   "source": [
    "After reviewing the MNIST dataset it is now time to train Random Forest Classifier on the given data and try the proposed approach from the paper. For that reason, `run_classification_experiment` function is defined to train the given model on training dataset and return zero-one loss calculated on testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23962a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classification_experiment(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    loss = zero_one_loss(y_test, y_pred)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d75d4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ITER = 10\n",
    "\n",
    "errors_rfc = []\n",
    "errors_rrf = []\n",
    "\n",
    "for iter in range(ITER):\n",
    "    # split data into training and testing dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=10000)\n",
    "    \n",
    "    # train regular RandomForestClassifier\n",
    "    rfc = RandomForestClassifier(n_estimators=100, min_samples_split=5, max_features='sqrt')\n",
    "    error_rfc = run_classification_experiment(rfc, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # train refined RandomForestClassifier\n",
    "    rrf = RefinedRandomForest(rfc, C = 0.01, n_prunings = 0)\n",
    "    error_rrf = run_classification_experiment(rrf, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # save errors for both models\n",
    "    errors_rfc.append(error_rfc)\n",
    "    errors_rrf.append(error_rrf)\n",
    "\n",
    "average_error_rfc = sum(errors_rfc)/ITER\n",
    "standard_deviation_rfc = np.std(errors_rfc)\n",
    "\n",
    "average_error_rrf = sum(errors_rrf)/ITER\n",
    "standard_deviation_rrf = np.std(errors_rrf)\n",
    "\n",
    "print('Regular Random Forest Classifier')\n",
    "print(f'Average error: {average_error_rfc}')\n",
    "print(f'Standard deviation: {standard_deviation_rfc}')\n",
    "\n",
    "print('Refined Random Forest Classifier')\n",
    "print(f'Average error: {average_error_rrf}')\n",
    "print(f'Standard deviation: {standard_deviation_rrf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17641ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "rrf.n_prunings = 1 # do one pruning in each iteration\n",
    "trainloss = []\n",
    "testloss = []\n",
    "nleaves = [sum(rrf.n_leaves_)] # remember number of leaves in each iteration\n",
    "for k in range(100):\n",
    "    rrf.fit(X_train, y_train) # fit and do 1 pruning\n",
    "    loss4tr = zero_one_loss(y_train, rrf.predict(X_train))\n",
    "    loss4te = zero_one_loss(y_test, rrf.predict(X_test))\n",
    "    trainloss.append(loss4tr)\n",
    "    testloss.append(loss4te)\n",
    "    nleaves.append(sum(rrf.n_leaves_))\n",
    "    print(\"Pruning {:02d}: train loss {:.4f}, test loss {:.4f}\".format(k+1, loss4tr, loss4te))\n",
    "    if loss4te > 0.48:\n",
    "        break # stop when test error increases too much\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12439a0c",
   "metadata": {},
   "source": [
    "After running these experiments it is visible that error drops when refined random forest fitted to the given MNIST datasets. It is also possible to execute pruning process on pre-trained random forest model to save space by reducing total number of leaves keeping the accuracy approximately equal. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7659c2",
   "metadata": {},
   "source": [
    "### 2.2 Classification: covtype "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2281bf42",
   "metadata": {},
   "source": [
    "This is the second example solving the classification problem and it includes predicting forest cover type from cartographic variables only (no remotely sensed data). The workflow is standard: \n",
    "1. importing given covtype dataset\n",
    "2. training standard random forest classifier\n",
    "3. training refined random forest classifier\n",
    "4. pruning refined random forest classifier\n",
    "\n",
    "Since this dataset is not accessible for import using standard libraries such as keras it will previously be downloaded from the source given in the **Introduction** and then read using common `read_csv` funtion from `pandas` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbcdcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_headers = pd.read_csv('./covtype/train.csv').columns[1:]\n",
    "trees = pd.read_csv(\"./covtype/covtype.data\", sep=\",\", header=None, names=column_headers)\n",
    "trees.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856697c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ITER = 10\n",
    "\n",
    "errors_rfc = []\n",
    "errors_rrf = []\n",
    "\n",
    "for iter in range(ITER):\n",
    "    # split data into training and testing dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(trees.drop(['Cover_Type'], axis='columns'), trees.Cover_Type, test_size=232405)    \n",
    "    \n",
    "    # train regular RandomForestClassifier\n",
    "    rfc = RandomForestClassifier(n_estimators=100, min_samples_split=5, max_features='sqrt')\n",
    "    error_rfc = run_classification_experiment(rfc, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # train refined RandomForestClassifier\n",
    "    rrf = RefinedRandomForest(rfc, C = 0.01, n_prunings = 0)\n",
    "    error_rrf = run_classification_experiment(rrf, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # save errors for both models\n",
    "    errors_rfc.append(error_rfc)\n",
    "    errors_rrf.append(error_rrf)\n",
    "\n",
    "average_error_rfc = sum(errors_rfc)/ITER\n",
    "standard_deviation_rfc = np.std(errors_rfc)\n",
    "\n",
    "average_error_rrf = sum(errors_rrf)/ITER\n",
    "standard_deviation_rrf = np.std(errors_rrf)\n",
    "\n",
    "print('Regular Random Forest Classifier')\n",
    "print(f'Average error: {average_error_rfc}')\n",
    "print(f'Standard deviation: {standard_deviation_rfc}')\n",
    "\n",
    "print('Refined Random Forest Classifier')\n",
    "print(f'Average error: {average_error_rrf}')\n",
    "print(f'Standard deviation: {standard_deviation_rrf}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6cc43a",
   "metadata": {},
   "source": [
    "### 2.3 Regression: abalone\n",
    "\n",
    "Here comes the second part of our **Standard machine learning tasks** where refined random forest will be tested on regression problems specifically abalone and ailerons tasks which will be described in details as we move on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e4b8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from URL using pandas read_csv method\n",
    "column_headers = ['Sex', 'Length', 'Diameter', 'Height', 'Whole weight', 'Shucked weight', 'Viscera weight', 'Shell weight', 'Rings']\n",
    "abalone = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data', header=None, names=column_headers)\n",
    "abalone.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45c3276",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender = {'I': 0, 'M': 1,'F': 2}\n",
    "abalone.Sex = [gender[item] for item in abalone.Sex]\n",
    "abalone.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d410cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_regression_experiment(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    error = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    return np.sqrt(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b9acbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ITER = 10\n",
    "\n",
    "errors_rfc = []\n",
    "errors_rrf = []\n",
    "\n",
    "for iter in range(ITER):\n",
    "    # split data into training and testing dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(abalone.drop(['Rings'], axis='columns'), abalone.Rings, test_size=1671)\n",
    "   \n",
    "    # train regular RandomForestRegressor\n",
    "    rfc = RandomForestRegressor(n_estimators=100, min_samples_split=5, max_features='sqrt')\n",
    "    error_rfc = run_regression_experiment(rfc, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # train refined RandomForestClassifier\n",
    "    rrf = RefinedRandomForest(rfc, C = 0.01, n_prunings = 0)\n",
    "    error_rrf = run_regression_experiment(rrf, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # save errors for both models\n",
    "    errors_rfc.append(error_rfc)\n",
    "    errors_rrf.append(error_rrf)\n",
    "\n",
    "average_error_rfc = sum(errors_rfc)/ITER\n",
    "standard_deviation_rfc = np.std(errors_rfc)\n",
    "\n",
    "average_error_rrf = sum(errors_rrf)/ITER\n",
    "standard_deviation_rrf = np.std(errors_rrf)\n",
    "\n",
    "print('Regular Random Forest Classifier')\n",
    "print(f'Average error: {average_error_rfc}')\n",
    "print(f'Standard deviation: {standard_deviation_rfc}')\n",
    "\n",
    "print('Refined Random Forest Classifier')\n",
    "print(f'Average error: {average_error_rrf}')\n",
    "print(f'Standard deviation: {standard_deviation_rrf}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d178ff3",
   "metadata": {},
   "source": [
    "### 2.4 Regression: ailerons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0d278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ailerons = pd.read_csv(\"./ailerons/ailerons.csv\")\n",
    "ailerons.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ed4fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ITER = 10\n",
    "\n",
    "errors_rfc = []\n",
    "errors_rrf = []\n",
    "\n",
    "for iter in range(ITER):\n",
    "    # split data into training and testing dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(ailerons.drop(['goal'], axis='columns'), ailerons.goal, test_size=6596)\n",
    "    \n",
    "    # train regular RandomForestClassifier\n",
    "    rfc = RandomForestClassifier(n_estimators=100, min_samples_split=5, max_features='sqrt')\n",
    "    error_rfc = run_regression_experiment(rfc, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # train refined RandomForestClassifier\n",
    "    rrf = RefinedRandomForest(rfc, C = 0.01, n_prunings = 0)\n",
    "    error_rrf = run_regression_experiment(rrf, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # save errors for both models\n",
    "    errors_rfc.append(error_rfc)\n",
    "    errors_rrf.append(error_rrf)\n",
    "\n",
    "average_error_rfc = sum(errors_rfc)/ITER\n",
    "standard_deviation_rfc = np.std(errors_rfc)\n",
    "\n",
    "average_error_rrf = sum(errors_rrf)/ITER\n",
    "standard_deviation_rrf = np.std(errors_rrf)\n",
    "\n",
    "print('Regular Random Forest Classifier')\n",
    "print(f'Average error: {average_error_rfc}')\n",
    "print(f'Standard deviation: {standard_deviation_rfc}')\n",
    "\n",
    "print('Refined Random Forest Classifier')\n",
    "print(f'Average error: {average_error_rrf}')\n",
    "print(f'Standard deviation: {standard_deviation_rrf}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd5c098",
   "metadata": {},
   "source": [
    "### 3. Application II: human age regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ac2a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
