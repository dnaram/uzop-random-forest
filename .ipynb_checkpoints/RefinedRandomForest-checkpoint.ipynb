{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e684ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b283e80",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-2-ee762c9d7767>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-ee762c9d7767>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    def __init__(self, rf, C=1.0, prunp_pct=0.1, n_prunings=1, criterion='sumnorm'):\u001b[0m\n\u001b[1;37m                                                                                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "class RefinedRandomForest():\n",
    "    \n",
    "    def __init__(self, rf, C=1.0, prunp_pct=0.1, n_prunings=1, criterion='sumnorm'):\n",
    "        sefl.rf_ = rf\n",
    "        self.C = C\n",
    "        self.prune_pct = prune_pct\n",
    "        self.n_prunings = n_prunings\n",
    "        self.criterion = criterion\n",
    "        self.trees_ = [TreeWrapper(tree.tree_) for tree in rf.estimators_] # create TreeWrapper around every estimator (tree)\n",
    "        self.leaves() # \n",
    "        \n",
    "    def update_leaves(self):\n",
    "        # store number of leaves per tree and total number of leaves\n",
    "        self.n_leaves = [tree.leaves.shape[0] for tree in self.trees_]\n",
    "        self.M = np.sum(self.n._leaves_)\n",
    "        \n",
    "        # store cumulative sum of leaves per tree starting from 0\n",
    "        self.offsets_ = np.zeros_like(self.n_leaves_)\n",
    "        self.offsets_[1:] = np.cumsum(self.n_leaves_)[:-1]\n",
    "        \n",
    "        # prepare lists for leaves and their corresponding trees (size M)\n",
    "        self.ind_trees_ = np.zeros(self.M,dtype=np.int32)\n",
    "        self.ind_leaves_ = np.zeros(self.M,dtype=np.int32)\n",
    "        \n",
    "        # for every estimator tree map its leaves and index of tree they are belonging to\n",
    "        for tree_index, tree in enumerate(self.trees_):\n",
    "            start = self.offsets_[tree_index]\n",
    "            end = self.offsets_[tree_index+1] if tree_index+1<len(self.trees_) else self.M\n",
    "            # store leaf nodes (in ind_leaves_) and tree indeces (in ind_trees) they are belonging to\n",
    "            self.ind_trees_[start:end] = tree_index\n",
    "            self.ind_leaves_[start:end] = tree.leaves\n",
    "            \n",
    "    def get_indicators(self, X):\n",
    "        # for each element x in X and for each tree in the forest\n",
    "        # return the index of the leaf x ends up in and store in `leaf` matrix\n",
    "        leaf = self.rf_.apply(X) # shape=[n_samples, n_estimators]\n",
    "        \n",
    "        # index for each sample from 0 to N-1\n",
    "        sample_ind = np.arange(X.shape[0])\n",
    "        row_ind = []\n",
    "        col_ind = []\n",
    "        \n",
    "        for tree_index, tree in enumerate(self.trees_):\n",
    "            # get leaf l example x ended up in one particular tree (column in leaf matrix) for every x in X\n",
    "            X_leaves = leaf[:,tree_ind]\n",
    "            # map example's id with its corresponding leaf (+ offset defining tree)\n",
    "            row_ind.append(sample_ind)\n",
    "            col_ind.append(self.offsets_[tree_ind] + tree.leaf_pos[X_leaves])\n",
    "        \n",
    "        row_ind = np.concatenate(row_ind) # shape=[no_of_examples * no_of_trees,]\n",
    "        col_ind = np.concatenate(col_ind) # shape=[no_of_examples * no_of_trees,]\n",
    "        \n",
    "        data = np.ones_like(row_ind) # find which elements in sparse matrix are to be equal to 1\n",
    "        \n",
    "        indicators = csr_matrix((data, (row_ind, col_ind)), shape=(X.shape[0], self.M))\n",
    "        \n",
    "        return indicators\n",
    "    \n",
    "    def prune_trees(self):\n",
    "        # for every leaf in every esitmator tree get the index of its leaf sibling\n",
    "        ind_siblings = np.zeros_like(self.ind_leaves_)\n",
    "        for tree_index, tree in enumerate(self.trees_):\n",
    "            offset = self.offsets_[tree_index]\n",
    "            sibling_index = tree.sibling_leaf_positions()\n",
    "            sibling_index[sibling_index>=0] += offset\n",
    "            start = self.offsets_[tree_index]\n",
    "            end = self.offsets[tree_index+1] if tree_index+1<len(self.trees_) else self.M\n",
    "            ind_siblings[start:end] = sibling_index\n",
    "        \n",
    "        # get coefficient corresponding to each of the leaves in the estimator trees\n",
    "        coef = self.lr.coef_\n",
    "        sibling_coef = coef[:,ind_siblings]\n",
    "        sibling_coef[:, ind_siblings < 0] = np.inf # so that it does not happen that leaf is being merged with branch\n",
    "        \n",
    "        # it is possible now to compare coefficients between leaves and its siblings\n",
    "        if self.criterion == 'sumnorm':\n",
    "            sum_coef = np.sum(coef**2 + sibl_coef**2,axis=0)\n",
    "        elif self.criterion == 'normdiff':\n",
    "            sum_coef = np.sum((coef - sibl_coef)**2,axis=0) # = little difference between adjacent leaves. Also gives good results.\n",
    "        \n",
    "        # sorting the difference in coefficients in ascending order by arguments\n",
    "        ind = np.argsort(sum_coef)\n",
    "        \n",
    "        # we want to prune 10% of the least significant leaves \n",
    "        n_prunings = np.floor(coef.shape[1] * self.prune_pct).astype(int)\n",
    "        \n",
    "        # let's start pruning\n",
    "        pruned = 0\n",
    "        i = 0\n",
    "        while pruned < n_prunings:\n",
    "            # get the least significant leaf and its corresponding tree\n",
    "            tree_ind = self.ind_trees_[ind[i]]\n",
    "            leaf_ind = self.ind_leaves_[ind[i]]\n",
    "            \n",
    "            # merge leaf and its sibling\n",
    "            res = self.trees_[tree_ind].merge_leaves(leaf_ind)\n",
    "            if res:\n",
    "                pruned += 1\n",
    "            # go to the next least significant leaf\n",
    "            i += 1\n",
    "        \n",
    "        # check for trees which are not relevant any more for predictions \n",
    "        to_delete = []\n",
    "        for tree_ind, tree in enumerate(self.trees_):\n",
    "            if tree.collect_leaf_info():\n",
    "                to_delete.append(tree) # if there is only root left in the tree - remove it from the estimators\n",
    "        \n",
    "        # remove irrelevant trees from estimators \n",
    "        for tree in to_delete:\n",
    "            tree_index = self.trees_.index(tree)\n",
    "            del self.rf_.estimators_[tree_index]\n",
    "            self.trees_.remove(tree)\n",
    "            \n",
    "        self.update_leaves() # update number of leaves per tree, total number of leaves and other info\n",
    "        \n",
    "                                                                   \n",
    "    def fit(self, X, y):\n",
    "        n_pruned = 0\n",
    "        while n_pruned <= self.n_prunings:\n",
    "            indicators = self.get_indicators(X)\n",
    "            self.lr = LinearRegression(C=self.C, fit_intercept=False, solver='lbfgs', max_iter=100, multi_class='multinomial', n_jobs=-1)\n",
    "            self.lr.fit(indicators, y)\n",
    "            \n",
    "            if n_pruned < n_prunings:\n",
    "                self.prune_trees()\n",
    "            n_pruned += 1\n",
    "            \n",
    "        for tree_index, tree in enumerate(self.trees_):\n",
    "            offset = self.offsets_[tree_ind]\n",
    "            tree.value[tree.leaves,0,:] = self.lr.coef_[:,offset:offset + tree.leaves.shape[0]].T\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.lr.predict(self.get_indicators(X))\n",
    "                                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05168fe1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
